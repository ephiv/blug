[
  {
    "id": "1",
    "title": "first post",
    "date": "2025-08-23",
    "excerpt": "markdown testing",
    "content": "# hello\n\nwe be testing **markdown**:\n\n## basics\n\n- **big black bold wide fat**\n- *slim jim micheal jackson*\n- `inline code`\n- [linky link link](https://example.com)\n\n```c\nint main() {\n    std::cout << \"Hello World\" << std::endl;\n}\n```\n\n> blockquote go brr\n\nok i hope it looks good and not stupid ok bye!!"
  },
  {
    "id": "2",
    "title": "weekdays",
    "date": "2025-08-24",
    "excerpt": "how yall getting days of the week using the date",
    "content": "```py\ndef dayOfWeek(year, month, day):\n    t = [0, 3, 2, 5, 0, 3, 5, 1, 4, 6, 2, 4]\n\n    if month < 3:\n        year -= 1\n\n    return (year + year // 4 - year // 100 + year // 400 + t[month - 1] + day) % 7\n\nmapping = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\nprint(mapping[dayOfWeek(2025, 8, 24)])\n```\n\nthis is called uhh the tomohiko sakamoto's algorithm.\nim too lazy to express your concepts in text so here's this.\n```py\nimport math\n\nC = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334] # days before month\n\ndef totaldays(y, m, d):\n    return 365*y + math.floor(y/4) - math.floor(y/100) + math.floor(y/400) + C[m] + (d - 1)\n\nprint(f\"Total days since 2025-08-24 if all years are non-leap years are: {totaldays(2025, 8, 24)}\")\n```\n```\n[fivy@archlinux playground]$ venv/bin/python main.py\nTotal days since 0000-01-01 if all years are non-leap years are: 739882\n```\nthis is correct, it has been 739882 days since 0000-01-01 if all years were common.\n```py\nimport math\n\nC = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334] # days before month\nt0 = []\n\nfor i in range(len(C)):\n    t0.append((C[i]-1) % 7)\n\nprint(t0)\n\ndef day(y, m, d):\n    return (y + math.floor(y/4) - math.floor(y/100) + math.floor(y/400) + t0[m] + d) % 7\n\nprint(f\"Weekday of date if all years are non-leap years are: {day(2025, 8, 24)}\")\n```\n```\n[fivy@archlinux playground]$ venv/bin/python main.py\n[6, 2, 2, 5, 0, 3, 5, 1, 4, 6, 2, 4]\nWeekday of date if all years are non-leap years are: 3\n```\nto convert the days since the first gregorian day, mod 7 the result, 365 = 1 mod 7 so 365*y = y mod 7 and C[m]-1 mod 7 is t0\n*we transpose the -1 to C[m] for the t0 because we count from day 1 to 31 (max)*\nnow we're gonna go back to the original code\nwhy `y -= m < 3`? cuz leap-year stupi-\nby using january as *undecember* and febuary as *duodecember* of the last year,\n```py\nconsec = [\n    mapping[dayOfWeek(2024, 2, 28)],\n    mapping[dayOfWeek(2024, 2, 29)],\n    mapping[dayOfWeek(2024, 3, 1)],\n    mapping[dayOfWeek(2024, 3, 2)]\n]\n\nprint(str(consec))\n```\n```\n[fivy@archlinux playground]$ venv/bin/python main.py\n['Wednesday', 'Thursday', 'Friday', 'Saturday']\n```\nit treats leap days correctly.\n\nbut one more thing, our list was `[6, 2, 2,...`, what is `[0, 3, 2,...`?\nit's compensation for the trick\n`6 + 1 = 0 mod 7`\n`2 + 1 = 3 mod 7`\n\nok bye bye"
  },
  {
    "id": "3",
    "title": "the roblox situation",
    "date": "2025-08-26",
    "excerpt": "ok im a bit late in the coverage but ive been following",
    "content": "## schlep ban\none day, schlep came to his account and saw to it has been banned (all his accounts were terminated)\na little thank you gift roblox sent him:\n\n<image src=https://upload.wikimedia.org/wikipedia/en/c/c6/Schlep%27s_cease-and-desist_letter.jpg width = 400>\n\nthis cease and desist is super pr because it didnt look professional\nrather, trying to be desperate, roblox lies to his face about things he didnt and shouldnt have violated\n\n## controversy\nroblox suffered from the social media coverage and was threatened by the population.\npopular youtubers left the star program\n\nand chief safety officer \"*anticipated*\" this and sold the roblox stock, gaining profit from robloxs $12B loss\n\n## lawsuit\nschlep prepared with a lawyer and [a talk with chris hansen](https://www.youtube.com/watch?v=vIcVPPOB8TQ)\n\nand louisiana attorney general also addressed the issues of roblox with child safety\n\n## responses\n1. roblox thinks they can win so they double down on the schlep situation by posting about vigilantes on their blog\n2. roblox triples down by posting another about vigilantes and ex-roblox employee referred to it in a web article\n3. roblox quadruples down by showing they're capable of moving and managing items as they're consistently banning the schlep shirts and schlep games. proving everyone's point that roblox is *protecting* predators since if they didnt, at least the minority of them wouldve got banned\n4. roblox quintuples down and posts a response video (couldnt find the link srry) and is shown to use ai, which either means that roblox is very very lazy or unprofessional, or that roblox thinks they're so powerful that they dont need to bother sitting with people with the lower status (community vs. corporation)\n\n## in the end\ni just hope that roblox does become safer and doesn't get banned in every country and get deleted (since roblox was my whole gaming life)\n\ni would just like for david bazsucki step down from being ceo and matt kaufman be fired so there would be more rational people holding up the platform"
  },
  {
    "id": "4",
    "title": "how to intuitively calculate the volume of a sphere",
    "date": "2025-08-27",
    "excerpt": "it's called rediscovering leave me alone-",
    "content": "## you all heard\nthat a sphere has $V = \\frac{4}{3} \\pi r^3$. yeah i don't know how so im tryna make it again\n\n## begin\n$R$: sphere radius\n\ncut the sphere so that along $x$ there are discs with varying radius\n\n$$\ndV = \\pi[f(x)^2]dx\n$$\n\nwhere $f(x)$ will represent the radius of the discs\n\n<image src=\"./images/sphere_segmentation.png\" width=400 alt=\"very crude drawing\">\n\n$x^2 + y^2 = R^2$ is a circle so $$f(x) = \\sqrt{R^2 - x^2}$$\n\nthe final result is:\n\n$$\nV = \\int_{-R}^{R}{\\pi(R^2-x^2)dx}\n$$\n\nlet's simplify:\n\n$$V = \\pi\\int_{-R}^{R}{(R^2-x^2)dx}$$\n$$V = \\pi[\\int_{-R}^{R}{R^2dx} - \\int_{-R}^{R}{x^2dx}]$$\n$$\\int_{-R}^{R}{R^2dx} = R^2(2R) = 2R^3$$\n$$\\int_{-R}^{R}{x^2dx} = \\frac{2R^3}{3}$$\n$$V = \\pi(2R^3 - \\frac{2R^3}{3}) = \\pi\\frac{4R^3}{3}$$\nthus becomes $V = \\frac{4}{3}\\pi r^2$"
  },
  {
    "id": "5",
    "title": "fnam3/llm (devlog I)",
    "date": "2025-08-29",
    "excerpt": "i've tried building llms before, so i want to settle it",
    "content": "## hello\nive failed miserably trying to replicate tiny lms in python, let alone be able to create an llm from scratch. only thing ive been able to do was to load a huggingface transformer and to api stuff, i never could build one\n\n(im on consumer hardware: laptop, specifically an hp probook running arch, used to run windows)\n\nso today i hope i will succeed if i try again, as well as log it in this devlog\n\n## learning\ni want to build a gpt\ngpts were introduced cuz of this [\"attention is all you need\"](https://arxiv.org/abs/1706.03762]) paper\nand im learning from a [datacamp lesson](https://app.datacamp.com/learn/courses/transformer-models-with-pytorch) (totally not a sponsor)\n\n<img src=./images/transformer_architecture.png width=400>\n\nstraightforwardly, there are 2 blocks: *encoder* and *decoder*\n\nthere are a few things that are not straightforward, unfortunately.\n\nwhat is a \"positional encoding\"?\n<img src=https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc88ffcf7-5fa5-4405-bd70-048cce0002cf_1669x777.png width=400>\nohh, just vector embedding then? ok\n\nso then why is the architecture uhh\n```\n       opb\n        ^\n        |\nblk -> blk\n ^      ^\n |      |\ninp    out\n```\nthe output is going... in??\nohh, this is an *autoregressive* model...\nthe fact that the label for $Outputs$ just looks SO untuitive is making my head go coo coo\n\nok, so why does it need *attention*? what is *attention*?\n\n```\nChatGPT: Attention is the hack humans do in real life—ignoring most things to focus on what matters—and the Transformer just shamelessly copied it.\n```\nahh, thanks for that.\nand also uhh this..\n$$Attention(Q,K,V) = softmax(\\frac{QK^⊤}{\\sqrt{d_k}})V$$\n\nwhy this fraction?\nwhere did d pop from?\n\n*\\*looking at chatgpt again\\**\n\nok so attention is for context building so the other parts of the block do the \"next token\" thingy\n\n> couldntve understood this if i didnt pull out chatgpt and watched datacamp the whole time\n\nwait what is feed-forwar-\n\n## pytorch time\n### transformers\nok so\n```py\nimport torch.nn as nn\n\nmodel = nn.Transformer(\n    d_model=512,\n    ...\n```\nwait hold up we didnt build a transformer...\nwe just used... nn's transformer instead.?\n\nso WHY DID WE LEARN ABOUT IT-\n\n---\n> shut up\n\nanyways\n```py\nimport torch.nn as nn\n\nmodel = nn.Transformer(\n    d_model=512,\n    nhead=8,\n    num_encoder_layers=6,\n    num_decoder_layers=6\n)\n\nprint(model)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\n```\nsome warnings...\n```\nTransformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)\n```\n### positional encoding\n\ni got to the next video and\npositional encoding is not vector embedding??\n\n\"**positional encoding**: token position + embedded vector -> positional encoding\"\n\nohh so the $d$ from last time is probably dimensions, we're doing it like this:\n\n```py\nimport torch\nimport math\nimport torch.nn as nn\n\nclass InputEmbeddings(nn.Module):\n    def __init__(self, vocab_size:int, d_model:int) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n```\nsee the `sqrt()`? they say it's standard practice :/\n\n```py\nembedding_layer = InputEmbeddings(vocab_size=10000, d_model=512)\nembedded_output = embedding_layer(torch.tensor([[1,2,3,4],[5,6,7,8]]))\nprint(embedded_output.shape)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\ntorch.Size([2, 4, 512])\n```\ni cant see why thisll give me insight as to whether or not it works\nsince we dont even have a tokenizer yet soo\n\n### positional embedding\n---\n$$PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n$$PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{model}}})$$\n\nsimply, cos for odd embedding values and sin for even embedding values (idk what embeddings values or from which layer he is talking abt)\n\n> 10000 looks very similar to the vocab_size but in the code its directly entered so i dont think so\n\n```py\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n```\nnow if you're like me, you'll look back into the last equation and think to yourself:\n\n> wtf is `div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))`\n\n$sin(pos \\cdot e^{T*-(ln(10^5/d_{model}))})$ it says\n$cos(pos \\cdot e^{T*-(ln(10^5/d_{model}))})$ it says WHAT THE FU-\n\nwhere is $2i$?!\n\n---\n> let's believe in our guy\n\nok ok let's believe him\n\n```py\nembedding_layer = InputEmbeddings(vocab_size=10000, d_model=512)\nembedded_output = embedding_layer(torch.tensor([[1,2,3,4],[5,6,7,8]]))\n\npos_encoding_layer = PositionalEncoding(d_model=512, max_seq_length=4)\npos_encoded_output = pos_encoding_layer(embedded_output)\nprint(pos_encoded_output.shape)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\ntorch.Size([2, 4, 512])\n```\n...this doesn't prove it works\nbut it is true..\n```py\nembedding_layer = InputEmbeddings(vocab_size=10000, d_model=512)\nembedded_output = embedding_layer(torch.tensor([[1,2,3,4],[5,6,7,8]]))\nprint(embedded_output.shape)\nprint(embedded_output)\n\npos_encoding_layer = PositionalEncoding(d_model=512, max_seq_length=4)\npos_encoded_output = pos_encoding_layer(embedded_output)\nprint(pos_encoded_output.shape)\nprint(pos_encoded_output)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\ntorch.Size([2, 4, 512])\ntensor([[[ 23.3169,  48.9458, -10.0815,  ...,  -0.8123,  16.2006, -10.7324],\n         [-48.7757, -33.4545,   7.7105,  ...,  -1.2710,   4.7042, -16.5172],\n         [-16.2887,  34.3895,   7.8329,  ...,   8.0364,  20.0585,  -6.0314],\n         [ 16.5740,  36.7811, -19.2416,  ..., -48.7405,  19.7014, -17.3186]],\n\n        [[ 14.1007,   6.8976,  -4.0806,  ...,  -7.1628,  21.7757, -18.5892],\n         [-32.4874, -11.4412,  14.4820,  ...,  51.4439, -10.9587,  21.4143],\n         [-15.1541, -16.7121, -68.2401,  ...,  -2.1972,  -1.5416,   2.2679],\n         [ 15.8648, -12.6221,   2.4659,  ...,  -5.4230,  27.8897,  15.9972]]],\n       grad_fn=<MulBackward0>)\ntorch.Size([2, 4, 512])\ntensor([[[ 23.3169,  49.9458, -10.0815,  ...,   0.1877,  16.2006,  -9.7324],\n         [-47.9342, -32.9142,   8.5323,  ...,  -0.2710,   4.7043, -15.5172],\n         [-15.3794,  33.9734,   8.7693,  ...,   9.0364,  20.0587,  -5.0314],\n         [ 16.7151,  35.7911, -18.9966,  ..., -47.7405,  19.7018, -16.3186]],\n\n        [[ 14.1007,   7.8976,  -4.0806,  ...,  -6.1628,  21.7757, -17.5892],\n         [-31.6460, -10.9009,  15.3038,  ...,  52.4439, -10.9586,  22.4143],\n         [-14.2448, -17.1283, -67.3037,  ...,  -1.1972,  -1.5414,   3.2679],\n         [ 16.0059, -13.6121,   2.7110,  ...,  -4.4230,  27.8901,  16.9972]]],\n       grad_fn=<AddBackward0>)\n```\nits working alr\npositional encoding is just really small kinda\n\n## ok bye"
  },
  {
    "id": "6",
    "title": "fnam3/llm (devlog II)",
    "date": "2025-08-30",
    "excerpt": "im gonna continue making the architecture",
    "content": "## hello\nwe left off at uhh\npositional encoding\n\nso now it's multi-head self-attention (still no idea wth that means)\n\n*\\*looking at datacamp vid\\**\n\nohh i see the q k and v thing we were showing last time\n\n**query**: *relations* with other tokens. i think it's better to say like what this token is *looking for* (from datacamp) cuz uhh **query** soo\n\n**key**: what is the content of this token. im surprised its derived and not what the whole encoding is about\n\n**values**: \"actual content to be aggregated or weighted\"... what is key and what is value? they're both same-d matrices with numbers in them, so k is not v???\n\nok here's the diagram uhh:\n```\n[n] token embeddings\n         |\n         |\n  +------+--------+------+\n  |               |      |\n  v               v      v\nquery            key   value\n  |               |\n  +---------------+\n   Q-K similarity\n  (typically calc\n   by dot-product\n         \"metric\")\n          |\n          v\n[nxn] attention scores\n          |\n          + softmax\n          |\n          v\n[nxn] attention weights\n```\n\nlooking back at the `Attention is the hack humans do in real life—ignoring most things to focus on what matters` i guess the attention weights are multiplied to the token for \"attention\"?\n\n```\n[n] token embeddings\n         |\n         |\n  +------+--------+------+\n  |               |      |\n  v               v      v\nquery            key   value\n  |               |      |\n  +---------------+      |\n   Q-K similarity        |\n  (typically calc        |\n   by dot-product        |\n         \"metric\")       |\n          |              |\n          v              |\n[nxn] attention scores   |\n          |              |\n          + softmax      |\n          |              |\n          v              |\n[nxn] attention weights--+--multiplied by V\n                         |\n                         v\n              updated token embedding\n```\n\nohh ok\nbut i still don't get how to use this attention cuz he demonstrated it on a complete sentence??\n\n### multi-head attention\nbecause a sentence is made up of tokens, attention has to be done for each. so *multi-head attention*\n\n*\\*looking at video again\\**\n\nthats wrong...\neach of them says uhh\n\"orange -> fruit\"\n\"positive sentiment\"\n\"subject: orange\"\nso it's not for different tokens?\nim blown.\n\nconcatenating attention heads?\nif they represent like \"orange ->  fruit\" and \"subject: orange\" how would we even concatenate them?\n\nremember the $softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$ i got from chatgpt? it exactly matches in here\n\n```py\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_head == 0, \"d_model must fit num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n        self.output_linear = nn.Linear(d_model, d_model)\n\n    def split_heads(self, x, batch_size):\n        seq_length = x.size(1)\n        x = x.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n        return x.permute(0, 2, 1, 3)\n\n    def compute_attention(self, query, key, value, mask=None):\n        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        attention_weights = F.softmax(scores, dim=-1)\n        return torch.matmul(attention_weights, value)\n\n    def combine_heads(self, x, batch_size):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        return x.view(batch_size, -1, self.d_model)\n```\n> not finished cuz no forward pass\n\n```py\ndef compute_attention(self, query, key, value, mask=None):\n    scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    attention_weights = F.softmax(scores, dim=-1)\n    return torch.matmul(attention_weights, value)\n```\nyeah seems about right, so $d_k$ is `head_dim`\n\n```py\ndef forward(self, query, key, value, mask=None):\n    batch_size = query.size(0)\n\n    query = self.split_heads(self.query_linear(query), batch_size)\n    key = self.split_heads(self.key_linear(key), batch_size)\n    value = self.split_heads(self.value_linear(value), batch_size)\n\n    attention_weights = self.compute_attention(query, key, value, mask)\n\n    output = self.combine_heads(attention_weights, batch_size)\n    return self.output_linear(output)\n```\ncool\n\n### encoder-only transformer\nok we're gonna make one yayyy\n\nso there's only an encoder to \"put emphasis on the context of the words\" and \"output predictions\"\n\nuhh ok\n\n```\n   input\n     |\n     v\n _______________________________\n| layer 1                       |\n|                               |\n| attention ----+               |\n|               |               |\n|               +- feed-forward |\n|                      |        |\n ______________________+________\n ______________________|________\n|  +-layer 2-----------+        |\n|  |                            |\n| attention ----+               |\n|               |               |\n|               +- feed-forward |\n|                      |        |\n ______________________+________\n ______________________|________\n|  +----[...] layer n--+        |\n|  |                            |\n| attention ----+               |\n|               |               |\n|               +- feed-forward |\n|                      |        |\n ______________________+________|\n| transformer head     |        |\n|                      |        |\n|                      v        |\n|          output predictions   |\n _______________________________\n```\nyeah it looks like that\n\n```py\nclass FeedForwardSubLayer(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n```\nin the architecture i actually forgot to add the norm layers following both attn and ff sub-layers so keep that in mind\n\n```py\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, src_mask):\n        attn_output = self.self_attn(x, x, x, src_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.ff_sublayer(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x\n```\nan encoder layer consists of `attention -> norm -> ff -> norm`\n```py\nclass TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n        super().__init__()\n\n        self.embedding = InputEmbeddings(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n        self.layers = nn.ModuleList(\n            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n        )\n\n    def forward(self, x, src_mask):\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        for layer in self.layers:\n            x = layer(x, src_mask)\n\n        return x\n```\n---\nnow 90% of the transformer is complete, we can choose to *plug* in 2 heads\n```py\nclass ClassifierHead(nn.Module):\n    def __init__(self, d_model, num_classes):\n        super().__init__()\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        logits = self.fc(x)\n        return F.log_softmax(logits, dim=-1)\n```\ntext classification, sentiment analysis will use this head often\n```py\nclass RegressionHead(nn.Module):\n    def __init__(self, d_model, output_dim):\n        super().__init__()\n        self.fc = nn.Linear(d_model, output_dim)\n\n    def forward(self, x):\n        return self.fc(x)\n```\nthis is a simpler head for scalar values like text readability and language complexity. probably used by the ai detectors out there\n\n#### cant test it yet cuz it aint trained\n\nTHERES A DECODER-ONLY TRANSFORMER-\n\n## next time"
  }
]
