[
  {
    "id": "1",
    "title": "first post",
    "date": "2025-08-23",
    "excerpt": "markdown testing",
    "content": "# hello\n\nwe be testing **markdown**:\n\n## basics\n\n- **big black bold wide fat**\n- *slim jim micheal jackson*\n- `inline code`\n- [linky link link](https://example.com)\n\n```c\nint main() {\n    std::cout << \"Hello World\" << std::endl;\n}\n```\n\n> blockquote go brr\n\nok i hope it looks good and not stupid ok bye!!"
  },
  {
    "id": "2",
    "title": "weekdays",
    "date": "2025-08-24",
    "excerpt": "how yall getting days of the week using the date",
    "content": "```py\ndef dayOfWeek(year, month, day):\n    t = [0, 3, 2, 5, 0, 3, 5, 1, 4, 6, 2, 4]\n\n    if month < 3:\n        year -= 1\n\n    return (year + year // 4 - year // 100 + year // 400 + t[month - 1] + day) % 7\n\nmapping = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\nprint(mapping[dayOfWeek(2025, 8, 24)])\n```\n\nthis is called uhh the tomohiko sakamoto's algorithm.\nim too lazy to express your concepts in text so here's this.\n```py\nimport math\n\nC = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334] # days before month\n\ndef totaldays(y, m, d):\n    return 365*y + math.floor(y/4) - math.floor(y/100) + math.floor(y/400) + C[m] + (d - 1)\n\nprint(f\"Total days since 2025-08-24 if all years are non-leap years are: {totaldays(2025, 8, 24)}\")\n```\n```\n[fivy@archlinux playground]$ venv/bin/python main.py\nTotal days since 0000-01-01 if all years are non-leap years are: 739882\n```\nthis is correct, it has been 739882 days since 0000-01-01 if all years were common.\n```py\nimport math\n\nC = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334] # days before month\nt0 = []\n\nfor i in range(len(C)):\n    t0.append((C[i]-1) % 7)\n\nprint(t0)\n\ndef day(y, m, d):\n    return (y + math.floor(y/4) - math.floor(y/100) + math.floor(y/400) + t0[m] + d) % 7\n\nprint(f\"Weekday of date if all years are non-leap years are: {day(2025, 8, 24)}\")\n```\n```\n[fivy@archlinux playground]$ venv/bin/python main.py\n[6, 2, 2, 5, 0, 3, 5, 1, 4, 6, 2, 4]\nWeekday of date if all years are non-leap years are: 3\n```\nto convert the days since the first gregorian day, mod 7 the result, 365 = 1 mod 7 so 365*y = y mod 7 and C[m]-1 mod 7 is t0\n*we transpose the -1 to C[m] for the t0 because we count from day 1 to 31 (max)*\nnow we're gonna go back to the original code\nwhy `y -= m < 3`? cuz leap-year stupi-\nby using january as *undecember* and febuary as *duodecember* of the last year,\n```py\nconsec = [\n    mapping[dayOfWeek(2024, 2, 28)],\n    mapping[dayOfWeek(2024, 2, 29)],\n    mapping[dayOfWeek(2024, 3, 1)],\n    mapping[dayOfWeek(2024, 3, 2)]\n]\n\nprint(str(consec))\n```\n```\n[fivy@archlinux playground]$ venv/bin/python main.py\n['Wednesday', 'Thursday', 'Friday', 'Saturday']\n```\nit treats leap days correctly.\n\nbut one more thing, our list was `[6, 2, 2,...`, what is `[0, 3, 2,...`?\nit's compensation for the trick\n`6 + 1 = 0 mod 7`\n`2 + 1 = 3 mod 7`\n\nok bye bye"
  },
  {
    "id": "3",
    "title": "the roblox situation",
    "date": "2025-08-26",
    "excerpt": "ok im a bit late in the coverage but ive been following",
    "content": "## schlep ban\none day, schlep came to his account and saw to it has been banned (all his accounts were terminated)\na little thank you gift roblox sent him:\n\n<image src=https://upload.wikimedia.org/wikipedia/en/c/c6/Schlep%27s_cease-and-desist_letter.jpg width = 400>\n\nthis cease and desist is super pr because it didnt look professional\nrather, trying to be desperate, roblox lies to his face about things he didnt and shouldnt have violated\n\n## controversy\nroblox suffered from the social media coverage and was threatened by the population.\npopular youtubers left the star program\n\nand chief safety officer \"*anticipated*\" this and sold the roblox stock, gaining profit from robloxs $12B loss\n\n## lawsuit\nschlep prepared with a lawyer and [a talk with chris hansen](https://www.youtube.com/watch?v=vIcVPPOB8TQ)\n\nand louisiana attorney general also addressed the issues of roblox with child safety\n\n## responses\n1. roblox thinks they can win so they double down on the schlep situation by posting about vigilantes on their blog\n2. roblox triples down by posting another about vigilantes and ex-roblox employee referred to it in a web article\n3. roblox quadruples down by showing they're capable of moving and managing items as they're consistently banning the schlep shirts and schlep games. proving everyone's point that roblox is *protecting* predators since if they didnt, at least the minority of them wouldve got banned\n4. roblox quintuples down and posts a response video (couldnt find the link srry) and is shown to use ai, which either means that roblox is very very lazy or unprofessional, or that roblox thinks they're so powerful that they dont need to bother sitting with people with the lower status (community vs. corporation)\n\n## in the end\ni just hope that roblox does become safer and doesn't get banned in every country and get deleted (since roblox was my whole gaming life)\n\ni would just like for david bazsucki step down from being ceo and matt kaufman be fired so there would be more rational people holding up the platform"
  },
  {
    "id": "4",
    "title": "how to intuitively calculate the volume of a sphere",
    "date": "2025-08-27",
    "excerpt": "it's called rediscovering leave me alone-",
    "content": "## you all heard\nthat a sphere has $V = \\frac{4}{3} \\pi r^3$. yeah i don't know how so im tryna make it again\n\n## begin\n$R$: sphere radius\n\ncut the sphere so that along $x$ there are discs with varying radius\n\n$$\ndV = \\pi[f(x)^2]dx\n$$\n\nwhere $f(x)$ will represent the radius of the discs\n\n<image src=\"./images/sphere_segmentation.png\" width=400 alt=\"very crude drawing\">\n\n$x^2 + y^2 = R^2$ is a circle so $$f(x) = \\sqrt{R^2 - x^2}$$\n\nthe final result is:\n\n$$\nV = \\int_{-R}^{R}{\\pi(R^2-x^2)dx}\n$$\n\nlet's simplify:\n\n$$V = \\pi\\int_{-R}^{R}{(R^2-x^2)dx}$$\n$$V = \\pi[\\int_{-R}^{R}{R^2dx} - \\int_{-R}^{R}{x^2dx}]$$\n$$\\int_{-R}^{R}{R^2dx} = R^2(2R) = 2R^3$$\n$$\\int_{-R}^{R}{x^2dx} = \\frac{2R^3}{3}$$\n$$V = \\pi(2R^3 - \\frac{2R^3}{3}) = \\pi\\frac{4R^3}{3}$$\nthus becomes $V = \\frac{4}{3}\\pi r^2$"
  },
  {
    "id": "5",
    "title": "fnam3/llm (devlog I)",
    "date": "2025-08-29",
    "excerpt": "i've tried building llms before, so i want to settle it",
    "content": "## hello\nive failed miserably trying to replicate tiny lms in python, let alone be able to create an llm from scratch. only thing ive been able to do was to load a huggingface transformer and to api stuff, i never could build one\n\n(im on consumer hardware: laptop, specifically an hp probook running arch, used to run windows)\n\nso today i hope i will succeed if i try again, as well as log it in this devlog\n\n## learning\ni want to build a gpt\ngpts were introduced cuz of this [\"attention is all you need\"](https://arxiv.org/abs/1706.03762]) paper\nand im learning from a [datacamp lesson](https://app.datacamp.com/learn/courses/transformer-models-with-pytorch) (totally not a sponsor)\n\n![](./images/transformer_architecture.png)\n\nstraightforwardly, there are 2 blocks: *encoder* and *decoder*\n\nthere are a few things that are not straightforward, unfortunately.\n\nwhat is a \"positional encoding\"?\n![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc88ffcf7-5fa5-4405-bd70-048cce0002cf_1669x777.png)\nohh, just vector embedding then? ok\n\nso then why is the architecture uhh\n```\n       opb\n        ^\n        |\nblk -> blk\n ^      ^\n |      |\ninp    out\n```\nthe output is going... in??\nohh, this is an *autoregressive* model...\nthe fact that the label for $Outputs$ just looks SO untuitive is making my head go coo coo\n\nok, so why does it need *attention*? what is *attention*?\n\n```\nChatGPT: Attention is the hack humans do in real life—ignoring most things to focus on what matters—and the Transformer just shamelessly copied it.\n```\nahh, thanks for that.\nand also uhh this..\n$$Attention(Q,K,V) = softmax(\\frac{QK^⊤}{\\sqrt{d_k}})V$$\n\nwhy this fraction?\nwhere did d pop from?\n\n*\\*looking at chatgpt again\\**\n\nok so attention is for context building so the other parts of the block do the \"next token\" thingy\n\n> couldntve understood this if i didnt pull out chatgpt and watched datacamp the whole time\n\nwait what is feed-forwar-\n\n## pytorch time\n### transformers\nok so\n```py\nimport torch.nn as nn\n\nmodel = nn.Transformer(\n    d_model=512,\n    ...\n```\nwait hold up we didnt build a transformer...\nwe just used... nn's transformer instead.?\n\nso WHY DID WE LEARN ABOUT IT-\n\n---\n> shut up\n\nanyways\n```py\nimport torch.nn as nn\n\nmodel = nn.Transformer(\n    d_model=512,\n    nhead=8,\n    num_encoder_layers=6,\n    num_decoder_layers=6\n)\n\nprint(model)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\n```\nsome warnings...\n```\nTransformer(\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)\n```\n### positional encoding\n\ni got to the next video and\npositional encoding is not vector embedding??\n\n\"**positional encoding**: token position + embedded vector -> positional encoding\"\n\nohh so the $d$ from last time is probably dimensions, we're doing it like this:\n\n```py\nimport torch\nimport math\nimport torch.nn as nn\n\nclass InputEmbeddings(nn.Module):\n    def __init__(self, vocab_size:int, d_model:int) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n```\nsee the `sqrt()`? they say it's standard practice :/\n\n```py\nembedding_layer = InputEmbeddings(vocab_size=10000, d_model=512)\nembedded_output = embedding_layer(torch.tensor([[1,2,3,4],[5,6,7,8]]))\nprint(embedded_output.shape)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\ntorch.Size([2, 4, 512])\n```\ni cant see why thisll give me insight as to whether or not it works\nsince we dont even have a tokenizer yet soo\n\n### positional embedding\n---\n$$PE_{(pos,2i)}=sin(\\frac{pos}{10000^{2i/d_{model}}})$$\n$$PE_{(pos,2i+1)}=cos(\\frac{pos}{10000^{2i/d_{model}}})$$\n\nsimply, cos for odd embedding values and sin for even embedding values (idk what embeddings values or from which layer he is talking abt)\n\n> 10000 looks very similar to the vocab_size but in the code its directly entered so i dont think so\n\n```py\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n```\nnow if you're like me, you'll look back into the last equation and think to yourself:\n\n> wtf is `div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))`\n\n$sin(pos \\cdot e^{T*-(ln(10^5/d_{model}))})$ it says\n$cos(pos \\cdot e^{T*-(ln(10^5/d_{model}))})$ it says WHAT THE FU-\n\nwhere is $2i$?!\n\n---\n> let's believe in our guy\n\nok ok let's believe him\n\n```py\nembedding_layer = InputEmbeddings(vocab_size=10000, d_model=512)\nembedded_output = embedding_layer(torch.tensor([[1,2,3,4],[5,6,7,8]]))\n\npos_encoding_layer = PositionalEncoding(d_model=512, max_seq_length=4)\npos_encoded_output = pos_encoding_layer(embedded_output)\nprint(pos_encoded_output.shape)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\ntorch.Size([2, 4, 512])\n```\n...this doesn't prove it works\nbut it is true..\n```py\nembedding_layer = InputEmbeddings(vocab_size=10000, d_model=512)\nembedded_output = embedding_layer(torch.tensor([[1,2,3,4],[5,6,7,8]]))\nprint(embedded_output.shape)\nprint(embedded_output)\n\npos_encoding_layer = PositionalEncoding(d_model=512, max_seq_length=4)\npos_encoded_output = pos_encoding_layer(embedded_output)\nprint(pos_encoded_output.shape)\nprint(pos_encoded_output)\n```\n```\n(venv) [fivy@archlinux llm]$ python main.py\ntorch.Size([2, 4, 512])\ntensor([[[ 23.3169,  48.9458, -10.0815,  ...,  -0.8123,  16.2006, -10.7324],\n         [-48.7757, -33.4545,   7.7105,  ...,  -1.2710,   4.7042, -16.5172],\n         [-16.2887,  34.3895,   7.8329,  ...,   8.0364,  20.0585,  -6.0314],\n         [ 16.5740,  36.7811, -19.2416,  ..., -48.7405,  19.7014, -17.3186]],\n\n        [[ 14.1007,   6.8976,  -4.0806,  ...,  -7.1628,  21.7757, -18.5892],\n         [-32.4874, -11.4412,  14.4820,  ...,  51.4439, -10.9587,  21.4143],\n         [-15.1541, -16.7121, -68.2401,  ...,  -2.1972,  -1.5416,   2.2679],\n         [ 15.8648, -12.6221,   2.4659,  ...,  -5.4230,  27.8897,  15.9972]]],\n       grad_fn=<MulBackward0>)\ntorch.Size([2, 4, 512])\ntensor([[[ 23.3169,  49.9458, -10.0815,  ...,   0.1877,  16.2006,  -9.7324],\n         [-47.9342, -32.9142,   8.5323,  ...,  -0.2710,   4.7043, -15.5172],\n         [-15.3794,  33.9734,   8.7693,  ...,   9.0364,  20.0587,  -5.0314],\n         [ 16.7151,  35.7911, -18.9966,  ..., -47.7405,  19.7018, -16.3186]],\n\n        [[ 14.1007,   7.8976,  -4.0806,  ...,  -6.1628,  21.7757, -17.5892],\n         [-31.6460, -10.9009,  15.3038,  ...,  52.4439, -10.9586,  22.4143],\n         [-14.2448, -17.1283, -67.3037,  ...,  -1.1972,  -1.5414,   3.2679],\n         [ 16.0059, -13.6121,   2.7110,  ...,  -4.4230,  27.8901,  16.9972]]],\n       grad_fn=<AddBackward0>)\n```\nits working alr\npositional encoding is just really small kinda\n\n## ok bye"
  }
]
